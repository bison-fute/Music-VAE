# Music VAE

The first time : 
* create a "data" folder, with in it the lmd_aligned folder (if CSV not already recovered).
* create an empty "midi" folder, in which will be written the CSV (if not already recovered).

**data used :** lmd-aligned from the Lahk MIDI Dataset

## Information, basic inspiration

 **Original idea**
https://www.jeremyjordan.me/variational-autoencoders/
https://magenta.tensorflow.org/music-vae
github: https://github.com/magenta/magenta/tree/master/magenta/models/music_vae

**A detailled notebook on the dataset** **A detailled notebook on the dataset **A detailled notebook on the dataset
https://nbviewer.jupyter.org/github/craffel/midi-dataset/blob/master/Tutorial.ipynb 

**Dataset**
https://colinraffel.com/projects/lmd/ 

Kaggle notebook on music generation **Kaggle notebook on music generation**
https://www.kaggle.com/basu369victor/generate-music-with-variational-autoencoder

Auto-encoder treated with midi files **Auto-encoder treated with midi files **Auto-encoder treated with midi files **Auto-encoder treated with midi files
Github: https://github.com/vikrosj/music-autoencoders
https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5
With Pytorch : https://github.com/Variational-Autoencoder/MusicVAE (IT IS FROM THIS GITHUB THAT THE CODES WERE COLLECTED, BE CAREFUL THERE ARE CHANGES: bad python version, mystical tree etc)

**Related doc on YouTube **Related doc on YouTube
Project with code : https://www.youtube.com/watch?v=0eTYs4n1LKg&list=LL&index=1&ab_channel=JCOpUntukIndonesia
Animated midi file: https://www.youtube.com/watch?v=gGqrVAe0_Ek&list=LL&index=5&ab_channel=JanAbraham
Project: https://www.youtube.com/watch?v=aOsET8KapQQ&list=LL&index=4&ab_channel=KieCodes

## To do

Test the influence of VAE parameters, code the concatenation of fading audio extracts.
